{
  "version": 2.0,
  "questions": [
    {
      "question": "Which output does the classifier in the experiment predict?",
      "answers": {
        "a": "Probability",
        "b": "Feature vector",
        "c": "Class label",
        "d": "Training data"
      },
      "explanations": {
        "a": "Probabilities are intermediate outputs, not the final prediction.",
        "b": "Feature vectors are inputs to the classifier.",
        "c": "The classifier predicts whether a message is spam or ham.",
        "d": "Training data is not an output of the model."
      },
      "correctAnswer": "c",
      "difficulty": "beginner"
    },
    {
      "question": "Why do we perform train-test split ?",
      "answers": {
        "a": "To ensure equal dataset size",
        "b": "Same class distribution in train and test sets",
        "c": "Faster training",
        "d": "Higher recall"
      },
      "explanations": {
        "a": "Equal dataset size is not the objective of splitting.",
        "b": "Stratified splitting preserves class distribution in both sets.",
        "c": "Splitting does not directly speed up training.",
        "d": "Recall depends on model performance, not data splitting."
      },
      "correctAnswer": "b",
      "difficulty": "beginner"
    },
    {
      "question": "How does class prior probability influence the final prediction in the Naive Bayes classifier used in the experiment?",
      "answers": {
        "a": "It removes irrelevant features from the dataset",
        "b": "It biases the prediction toward classes that occur more frequently in the training data",
        "c": "It converts text data into numerical vectors",
        "d": "It eliminates the need for likelihood estimation"
      },
      "explanations": {
        "a": "Class priors do not perform feature selection.",
        "b": "Higher prior probability increases the posterior probability of that class.",
        "c": "Vectorization is handled by techniques like TF-IDF.",
        "d": "Likelihood estimation is essential in Naive Bayes."
      },
      "correctAnswer": "b",
      "difficulty": "beginner"
    },
    {
      "question": "Why does Multinomial Naive Bayes use logarithmic probabilities during classification?",
      "answers": {
        "a": "To increase model accuracy",
        "b": "To simplify model interpretation",
        "c": "To avoid numerical underflow when multiplying small probabilities",
        "d": "To remove the need for smoothing"
      },
      "explanations": {
        "a": "Logarithms do not directly increase accuracy.",
        "b": "Logarithms are used for numerical stability, not interpretation.",
        "c": "Multiplying many small probabilities can cause underflow errors.",
        "d": "Smoothing is still required."
      },
      "correctAnswer": "c",
      "difficulty": "intermediate"
    },
    {
      "question": "During preprocessing, text is converted to lowercase, punctuation is removed, and extra spaces are eliminated. How do these steps improve the performance of the Naive Bayes classifier?",
      "answers": {
        "a": "They increase the size of the vocabulary",
        "b": "They ensure that different forms of the same word are treated consistently",
        "c": "They introduce additional features",
        "d": "They reduce the number of class labels"
      },
      "explanations": {
        "a": "Preprocessing usually reduces vocabulary size.",
        "b": "Normalization avoids treating the same word as different tokens.",
        "c": "Preprocessing removes noise rather than adding features.",
        "d": "Class labels remain unchanged."
      },
      "correctAnswer": "b",
      "difficulty": "intermediate"
    },
    {
      "question": "The experiment uses Multinomial Naive Bayes rather than Gaussian Naive Bayes. Which explanation best justifies this choice for text classification?",
      "answers": {
        "a": "Multinomial Naive Bayes assumes normally distributed features",
        "b": "Multinomial Naive Bayes works effectively with frequency-based and count-like text features",
        "c": "Gaussian Naive Bayes performs better with sparse matrices",
        "d": "Gaussian Naive Bayes does not support binary classification"
      },
      "explanations": {
        "a": "Normal distribution is assumed by Gaussian Naive Bayes.",
        "b": "Text data is represented using word counts or TF-IDF values.",
        "c": "Gaussian Naive Bayes is not suited for sparse text data.",
        "d": "Gaussian Naive Bayes supports binary classification."
      },
      "correctAnswer": "b",
      "difficulty": "intermediate"
    },
    {
      "question": "In the context of the confusion matrix generated in the experiment, what does a false positive indicate?",
      "answers": {
        "a": "A spam message correctly classified as spam",
        "b": "A ham message incorrectly classified as spam",
        "c": "A spam message incorrectly classified as ham",
        "d": "A ham message correctly classified as ham"
      },
      "explanations": {
        "a": "This represents a true positive.",
        "b": "A non-spam message is wrongly predicted as spam.",
        "c": "This represents a false negative.",
        "d": "This represents a true negative."
      },
      "correctAnswer": "b",
      "difficulty": "intermediate"
    },
    {
      "question": "In the spam detection experiment, which algorithm is used to classify messages as spam or ham?",
      "answers": {
        "a": "Decision Tree",
        "b": "Gaussian Na√Øve Bayes",
        "c": "Multinomial Naive Bayes",
        "d": "K-Means Clustering"
      },
      "explanations": {
        "a": "Decision Trees are not used in this experiment.",
        "b": "Gaussian Naive Bayes is unsuitable for text data.",
        "c": "Multinomial Naive Bayes handles word frequency features effectively.",
        "d": "K-Means is an unsupervised clustering algorithm."
      },
      "correctAnswer": "c",
      "difficulty": "beginner"
    },
    {
      "question": "Which technique is used in the experiment to convert text messages into feature vectors?",
      "answers": {
        "a": "One-Hot Encoding",
        "b": "TF-IDF Vectorization",
        "c": "Label Encoding",
        "d": "Image Scaling"
      },
      "explanations": {
        "a": "One-hot encoding is not efficient for large vocabularies.",
        "b": "TF-IDF converts text into weighted numerical features.",
        "c": "Label encoding is used for target variables.",
        "d": "Image scaling applies only to image data."
      },
      "correctAnswer": "b",
      "difficulty": "beginner"
    },
    {
      "question": "Based on the overall experiment, why is Naive Bayes considered a strong baseline model for text classification research and applications?",
      "answers": {
        "a": "It requires no feature engineering",
        "b": "It is computationally efficient, interpretable, and performs well on high-dimensional sparse data",
        "c": "It always outperforms deep learning models",
        "d": "It does not require labelled data"
      },
      "explanations": {
        "a": "Feature extraction like TF-IDF is still required.",
        "b": "Naive Bayes is fast, simple, and effective for text data.",
        "c": "Deep learning models can outperform Naive Bayes in many cases.",
        "d": "Naive Bayes requires labelled training data."
      },
      "correctAnswer": "b",
      "difficulty": "intermediate"
    }
  ]
}